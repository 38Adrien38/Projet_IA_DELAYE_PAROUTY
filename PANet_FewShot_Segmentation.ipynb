{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANet - Few-Shot Segmentation pour CT-Scans Médicaux\n",
    "\n",
    "**Date:** Janvier 2026  \n",
    "**Score obtenu:** 0.32 (Top 3: 0.34)\n",
    "\n",
    "## Description\n",
    "\n",
    "Ce notebook implémente PANet (Prototype Alignment Network) pour la segmentation few-shot de CT-scans médicaux.\n",
    "\n",
    "**Caractéristiques:**\n",
    "- 105 classes de structures anatomiques\n",
    "- Entraînement from scratch (sans modèle pré-entraîné)\n",
    "- Configuration 5-way 5-shot\n",
    "- ResNet-18 comme encodeur\n",
    "\n",
    "**Référence:**  \n",
    "Wang et al., \"PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment\", ICCV 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Vérification GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration du projet PANet.\"\"\"\n",
    "    \n",
    "    # Chemins (à adapter selon votre environnement)\n",
    "    train_images: Path = Path(\"data/X_train/images\")\n",
    "    test_images: Path = Path(\"data/X_test/images\")\n",
    "    train_labels: Path = Path(\"data/Y_train.csv\")\n",
    "    output_dir: Path = Path(\"outputs\")\n",
    "    \n",
    "    # Modèle\n",
    "    encoder_name: str = \"resnet18\"\n",
    "    feature_dim: int = 256\n",
    "    use_multiscale: bool = True\n",
    "    \n",
    "    # Entraînement épisodique\n",
    "    n_way: int = 5\n",
    "    k_shot: int = 5  # 5-shot pour de meilleurs résultats\n",
    "    n_query: int = 5\n",
    "    \n",
    "    # Optimisation\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    num_epochs: int = 50\n",
    "    train_episodes: int = 10000\n",
    "    val_episodes: int = 500\n",
    "    patience: int = 15  # Early stopping\n",
    "    \n",
    "    # Images\n",
    "    input_size: Tuple[int, int] = (256, 256)\n",
    "    original_size: Tuple[int, int] = (512, 512)\n",
    "    num_classes: int = 105\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42\n",
    "\n",
    "config = Config()\n",
    "print(f\"Configuration: {config.n_way}-way {config.k_shot}-shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encodeur ResNet-18 (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Bloc résiduel de base pour ResNet-18.\"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"ResNet-18 encodeur adapté pour images grayscale.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Couche initiale\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Couches résiduelles\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        # Projections vers dimension commune\n",
    "        self.proj2 = nn.Conv2d(128, feature_dim, kernel_size=1)\n",
    "        self.proj3 = nn.Conv2d(256, feature_dim, kernel_size=1)\n",
    "        self.proj4 = nn.Conv2d(512, feature_dim, kernel_size=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = [BasicBlock(self.in_planes, planes, stride, downsample)]\n",
    "        self.in_planes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, return_all_features=False):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer1(x)\n",
    "        f2 = self.proj2(self.layer2(x))\n",
    "        f3 = self.proj3(self.layer3(self.layer2(x)))\n",
    "        f4 = self.proj4(self.layer4(self.layer3(self.layer2(x))))\n",
    "\n",
    "        if return_all_features:\n",
    "            return {\"layer2\": f2, \"layer3\": f3, \"layer4\": f4}\n",
    "        return f4\n",
    "\n",
    "\n",
    "def get_encoder(in_channels=1, feature_dim=256):\n",
    "    return ResNetEncoder(in_channels=in_channels, feature_dim=feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PANet - Prototype Alignment Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeComputation(nn.Module):\n",
    "    \"\"\"Calcul des prototypes par masked average pooling.\"\"\"\n",
    "\n",
    "    def forward(self, support_features, support_masks, class_ids):\n",
    "        N, C, H, W = support_features.shape\n",
    "        _, H_mask, W_mask = support_masks.shape\n",
    "\n",
    "        if (H, W) != (H_mask, W_mask):\n",
    "            support_masks = F.interpolate(\n",
    "                support_masks.unsqueeze(1).float(),\n",
    "                size=(H, W), mode=\"nearest\"\n",
    "            ).squeeze(1).long()\n",
    "\n",
    "        prototypes = {}\n",
    "        for class_id in class_ids:\n",
    "            binary_mask = (support_masks == class_id).float()\n",
    "            masked_features = support_features * binary_mask.unsqueeze(1)\n",
    "            sum_features = masked_features.sum(dim=(0, 2, 3))\n",
    "            num_pixels = binary_mask.sum() + 1e-6\n",
    "            prototypes[class_id] = sum_features / num_pixels\n",
    "\n",
    "        return prototypes\n",
    "\n",
    "\n",
    "class PrototypeMatching(nn.Module):\n",
    "    \"\"\"Classification par distance cosine aux prototypes.\"\"\"\n",
    "\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, query_features, prototypes):\n",
    "        B, C, H, W = query_features.shape\n",
    "        class_ids = sorted(prototypes.keys())\n",
    "\n",
    "        proto_stack = torch.stack([prototypes[c] for c in class_ids], dim=0)\n",
    "        features_flat = query_features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "\n",
    "        features_norm = F.normalize(features_flat, p=2, dim=-1)\n",
    "        proto_norm = F.normalize(proto_stack, p=2, dim=-1)\n",
    "        scores = torch.matmul(features_norm, proto_norm.t())\n",
    "        scores = scores / self.temperature\n",
    "        scores = scores.permute(0, 2, 1).reshape(B, len(class_ids), H, W)\n",
    "\n",
    "        return scores, class_ids\n",
    "\n",
    "\n",
    "class PANet(nn.Module):\n",
    "    \"\"\"PANet complet pour segmentation few-shot.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, feature_dim=256, use_multiscale=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.feature_dim = feature_dim\n",
    "        self.use_multiscale = use_multiscale\n",
    "\n",
    "        self.prototype_computation = PrototypeComputation()\n",
    "        self.prototype_matching = PrototypeMatching(temperature=1.0)\n",
    "\n",
    "        if use_multiscale:\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Conv2d(feature_dim * 3, feature_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(feature_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def _extract_features(self, images):\n",
    "        features = self.encoder(images, return_all_features=True)\n",
    "        if self.use_multiscale:\n",
    "            target_size = features[\"layer2\"].shape[2:]\n",
    "            upsampled = []\n",
    "            for name in [\"layer2\", \"layer3\", \"layer4\"]:\n",
    "                feat = features[name]\n",
    "                if feat.shape[2:] != target_size:\n",
    "                    feat = F.interpolate(feat, size=target_size, mode=\"bilinear\", align_corners=False)\n",
    "                upsampled.append(feat)\n",
    "            return self.fusion(torch.cat(upsampled, dim=1))\n",
    "        return features[\"layer4\"]\n",
    "\n",
    "    def forward(self, support_images, support_masks, query_images, query_masks, class_ids):\n",
    "        support_features = self._extract_features(support_images)\n",
    "        query_features = self._extract_features(query_images)\n",
    "\n",
    "        prototypes = self.prototype_computation(support_features, support_masks, class_ids)\n",
    "        scores, ordered_classes = self.prototype_matching(query_features, prototypes)\n",
    "\n",
    "        # Loss de segmentation\n",
    "        target_size = scores.shape[2:]\n",
    "        query_masks_resized = F.interpolate(\n",
    "            query_masks.unsqueeze(1).float(), size=target_size, mode=\"nearest\"\n",
    "        ).squeeze(1).long()\n",
    "\n",
    "        class_to_idx = {c: i for i, c in enumerate(ordered_classes)}\n",
    "        remapped_masks = torch.full_like(query_masks_resized, -1)\n",
    "        for class_id in class_ids:\n",
    "            if class_id in class_to_idx:\n",
    "                remapped_masks[query_masks_resized == class_id] = class_to_idx[class_id]\n",
    "\n",
    "        loss = F.cross_entropy(scores, remapped_masks, ignore_index=-1)\n",
    "\n",
    "        # Prédictions\n",
    "        pred_indices = scores.argmax(dim=1)\n",
    "        idx_to_class = {i: c for i, c in enumerate(ordered_classes)}\n",
    "        predictions = torch.zeros_like(pred_indices)\n",
    "        for idx, class_id in idx_to_class.items():\n",
    "            predictions[pred_indices == idx] = class_id\n",
    "\n",
    "        if predictions.shape[1:] != query_masks.shape[1:]:\n",
    "            predictions = F.interpolate(\n",
    "                predictions.unsqueeze(1).float(), size=query_masks.shape[1:], mode=\"nearest\"\n",
    "            ).squeeze(1).long()\n",
    "\n",
    "        return loss, predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, support_images, support_masks, query_images, class_ids, output_size=None):\n",
    "        self.eval()\n",
    "        support_features = self._extract_features(support_images)\n",
    "        query_features = self._extract_features(query_images)\n",
    "\n",
    "        prototypes = self.prototype_computation(support_features, support_masks, class_ids)\n",
    "        scores, ordered_classes = self.prototype_matching(query_features, prototypes)\n",
    "\n",
    "        pred_indices = scores.argmax(dim=1)\n",
    "        idx_to_class = {i: c for i, c in enumerate(ordered_classes)}\n",
    "        predictions = torch.zeros_like(pred_indices)\n",
    "        for idx, class_id in idx_to_class.items():\n",
    "            predictions[pred_indices == idx] = class_id\n",
    "\n",
    "        if output_size is not None:\n",
    "            predictions = F.interpolate(\n",
    "                predictions.unsqueeze(1).float(), size=output_size, mode=\"nearest\"\n",
    "            ).squeeze(1).long()\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset CT-Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(csv_path, limit=None):\n",
    "    \"\"\"Charge les annotations depuis le CSV.\"\"\"\n",
    "    print(f\"Chargement des annotations depuis {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "    \n",
    "    if limit:\n",
    "        df = df.iloc[:, :limit]\n",
    "    \n",
    "    masks = {}\n",
    "    class_set = set()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        values = df[col].values\n",
    "        h = w = int(np.sqrt(len(values)))\n",
    "        mask = values.reshape(h, w).astype(np.int16)\n",
    "        masks[col] = mask\n",
    "        class_set.update(np.unique(mask))\n",
    "    \n",
    "    print(f\"  {len(masks)} images chargées\")\n",
    "    print(f\"  {len(class_set)} classes trouvées\")\n",
    "    \n",
    "    return masks, sorted(class_set)\n",
    "\n",
    "\n",
    "class CTScanDataset(Dataset):\n",
    "    \"\"\"Dataset pour CT-scans avec masques de segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, masks, target_size=(256, 256)):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.masks = masks\n",
    "        self.target_size = target_size\n",
    "        self.image_names = sorted(masks.keys())\n",
    "        \n",
    "        # Index classe -> images\n",
    "        self.class_to_images = {}\n",
    "        for name, mask in masks.items():\n",
    "            for c in np.unique(mask):\n",
    "                if c > 0:\n",
    "                    self.class_to_images.setdefault(c, []).append(name)\n",
    "        \n",
    "        self.available_classes = sorted(self.class_to_images.keys())\n",
    "        print(f\"Dataset initialisé: {len(self.image_names)} images, {len(self.available_classes)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.image_names[idx]\n",
    "        \n",
    "        # Charger image\n",
    "        img_path = self.image_dir / name\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        image = image.resize(self.target_size, Image.BILINEAR)\n",
    "        image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "        image_np = (image_np - 0.5) / 0.5\n",
    "        \n",
    "        # Charger masque\n",
    "        mask = self.masks[name]\n",
    "        mask_pil = Image.fromarray(mask.astype(np.int32))\n",
    "        mask_pil = mask_pil.resize(self.target_size, Image.NEAREST)\n",
    "        mask_np = np.array(mask_pil, dtype=np.int64)\n",
    "        \n",
    "        return {\n",
    "            'image': torch.from_numpy(image_np).unsqueeze(0),\n",
    "            'mask': torch.from_numpy(mask_np),\n",
    "            'name': name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sampler Épisodique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicSampler(Sampler):\n",
    "    \"\"\"Sampler pour entraînement épisodique N-way K-shot.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, n_way=5, k_shot=5, n_query=5, episodes=1000):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        # Filtrer classes avec assez d'images\n",
    "        min_images = k_shot + n_query\n",
    "        self.valid_classes = [\n",
    "            c for c, imgs in dataset.class_to_images.items()\n",
    "            if len(imgs) >= min_images\n",
    "        ]\n",
    "        print(f\"EpisodicSampler: {len(self.valid_classes)} classes valides, {n_way}-way {k_shot}-shot\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.episodes):\n",
    "            if len(self.valid_classes) < self.n_way:\n",
    "                classes = random.choices(self.valid_classes, k=self.n_way)\n",
    "            else:\n",
    "                classes = random.sample(self.valid_classes, self.n_way)\n",
    "            \n",
    "            support_indices, query_indices = [], []\n",
    "            \n",
    "            for c in classes:\n",
    "                images = self.dataset.class_to_images[c]\n",
    "                selected = random.sample(images, min(len(images), self.k_shot + self.n_query))\n",
    "                \n",
    "                for img_name in selected[:self.k_shot]:\n",
    "                    support_indices.append(self.dataset.image_names.index(img_name))\n",
    "                for img_name in selected[self.k_shot:self.k_shot + self.n_query]:\n",
    "                    query_indices.append(self.dataset.image_names.index(img_name))\n",
    "            \n",
    "            yield {'support': support_indices, 'query': query_indices, 'classes': classes}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred, target, num_classes=105):\n",
    "    \"\"\"Calcule le IoU moyen.\"\"\"\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred == c)\n",
    "        target_c = (target == c)\n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = (pred_c | target_c).sum().float()\n",
    "        if union > 0:\n",
    "            ious.append((intersection / union).item())\n",
    "    return np.mean(ious) if ious else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, sampler, optimizer, device):\n",
    "    \"\"\"Entraîne le modèle pendant une epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_iou = 0, 0\n",
    "    \n",
    "    for episode in tqdm(sampler, desc=\"Training\"):\n",
    "        support_data = [dataset[i] for i in episode['support']]\n",
    "        query_data = [dataset[i] for i in episode['query']]\n",
    "        \n",
    "        support_images = torch.stack([d['image'] for d in support_data]).to(device)\n",
    "        support_masks = torch.stack([d['mask'] for d in support_data]).to(device)\n",
    "        query_images = torch.stack([d['image'] for d in query_data]).to(device)\n",
    "        query_masks = torch.stack([d['mask'] for d in query_data]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, predictions = model(support_images, support_masks, query_images, query_masks, episode['classes'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_iou += compute_iou(predictions.cpu(), query_masks.cpu())\n",
    "    \n",
    "    n = len(sampler)\n",
    "    return total_loss / n, total_iou / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataset, sampler, device):\n",
    "    \"\"\"Valide le modèle.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_iou = 0, 0\n",
    "    \n",
    "    for episode in tqdm(sampler, desc=\"Validation\"):\n",
    "        support_data = [dataset[i] for i in episode['support']]\n",
    "        query_data = [dataset[i] for i in episode['query']]\n",
    "        \n",
    "        support_images = torch.stack([d['image'] for d in support_data]).to(device)\n",
    "        support_masks = torch.stack([d['mask'] for d in support_data]).to(device)\n",
    "        query_images = torch.stack([d['image'] for d in query_data]).to(device)\n",
    "        query_masks = torch.stack([d['mask'] for d in query_data]).to(device)\n",
    "        \n",
    "        loss, predictions = model(support_images, support_masks, query_images, query_masks, episode['classes'])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_iou += compute_iou(predictions.cpu(), query_masks.cpu())\n",
    "    \n",
    "    n = len(sampler)\n",
    "    return total_loss / n, total_iou / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Boucle d'Entraînement Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"Entraîne le modèle PANet.\"\"\"\n",
    "    \n",
    "    # Seed\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "    \n",
    "    # Créer le modèle\n",
    "    encoder = get_encoder(in_channels=1, feature_dim=config.feature_dim)\n",
    "    model = PANet(encoder, feature_dim=config.feature_dim, use_multiscale=config.use_multiscale)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    print(f\"Paramètres: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Charger les données\n",
    "    masks, classes = load_annotations(config.train_labels)\n",
    "    \n",
    "    # Split train/val\n",
    "    all_names = list(masks.keys())\n",
    "    random.shuffle(all_names)\n",
    "    split = int(0.8 * len(all_names))\n",
    "    \n",
    "    train_masks = {n: masks[n] for n in all_names[:split]}\n",
    "    val_masks = {n: masks[n] for n in all_names[split:]}\n",
    "    \n",
    "    train_dataset = CTScanDataset(config.train_images, train_masks, config.input_size)\n",
    "    val_dataset = CTScanDataset(config.train_images, val_masks, config.input_size)\n",
    "    \n",
    "    train_sampler = EpisodicSampler(train_dataset, config.n_way, config.k_shot, config.n_query, config.train_episodes)\n",
    "    val_sampler = EpisodicSampler(val_dataset, config.n_way, config.k_shot, config.n_query, config.val_episodes)\n",
    "    \n",
    "    # Optimisation\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
    "    \n",
    "    # Entraînement\n",
    "    best_iou = 0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_iou': [], 'val_loss': [], 'val_iou': []}\n",
    "    \n",
    "    output_dir = config.output_dir / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{config.num_epochs}\")\n",
    "        \n",
    "        train_loss, train_iou = train_epoch(model, train_dataset, train_sampler, optimizer, DEVICE)\n",
    "        val_loss, val_iou = validate(model, val_dataset, val_sampler, DEVICE)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train IoU: {train_iou*100:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val IoU: {val_iou*100:.2f}%\")\n",
    "        \n",
    "        # Sauvegarder meilleur modèle\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_iou': val_iou,\n",
    "            }, output_dir / 'best_model.pt')\n",
    "            print(f\"  -> Meilleur modèle sauvegardé (IoU: {val_iou*100:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"\\nEarly stopping après {epoch} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Sauvegarder historique\n",
    "        with open(output_dir / 'history.json', 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nEntraînement terminé! Meilleur IoU: {best_iou*100:.2f}%\")\n",
    "    return model, output_dir\n",
    "\n",
    "\n",
    "# Décommenter pour lancer l'entraînement\n",
    "# model, output_dir = train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prédiction et Soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset pour images de test.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, target_size=(256, 256)):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.target_size = target_size\n",
    "        self.image_paths = sorted(\n",
    "            list(self.image_dir.glob(\"*.png\")) + list(self.image_dir.glob(\"*.jpg\")),\n",
    "            key=lambda p: int(\"\".join(c for c in p.stem if c.isdigit()) or 0)\n",
    "        )\n",
    "        print(f\"TestDataset: {len(self.image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        image = Image.open(path).convert('L')\n",
    "        image = image.resize(self.target_size, Image.BILINEAR)\n",
    "        image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "        image_np = (image_np - 0.5) / 0.5\n",
    "        return {\n",
    "            'image': torch.from_numpy(image_np).unsqueeze(0),\n",
    "            'name': path.name,\n",
    "        }\n",
    "\n",
    "\n",
    "def generate_submission(model, test_dataset, support_images, support_masks, class_ids, output_path, device):\n",
    "    \"\"\"Génère le fichier de soumission.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    support_images = support_images.to(device)\n",
    "    support_masks = support_masks.to(device)\n",
    "    \n",
    "    for idx in tqdm(range(len(test_dataset)), desc=\"Prédiction\"):\n",
    "        sample = test_dataset[idx]\n",
    "        query_image = sample['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        pred = model.predict(support_images, support_masks, query_image, class_ids, output_size=(512, 512))\n",
    "        predictions[sample['name']] = pred[0].cpu().numpy().astype(np.int16)\n",
    "    \n",
    "    # Créer CSV\n",
    "    sorted_names = sorted(predictions.keys(), key=lambda n: int(\"\".join(c for c in n if c.isdigit()) or 0))\n",
    "    data = {name: predictions[name].flatten() for name in sorted_names}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.index = [f\"Pixel {i}\" for i in range(512*512)]\n",
    "    df.to_csv(output_path)\n",
    "    \n",
    "    print(f\"Soumission sauvegardée: {output_path}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# test_dataset = TestDataset(config.test_images, config.input_size)\n",
    "# generate_submission(model, test_dataset, support_images, support_masks, class_ids, 'submission.csv', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Chargement d'un Modèle Existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    \"\"\"Charge un modèle pré-entraîné.\"\"\"\n",
    "    encoder = get_encoder(in_channels=1, feature_dim=256)\n",
    "    model = PANet(encoder, feature_dim=256, use_multiscale=True)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Modèle chargé: {model_path}\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', '?')}\")\n",
    "    print(f\"  Val IoU: {checkpoint.get('val_iou', 0)*100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Exemple:\n",
    "# model = load_model('outputs/best_model.pt', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résultats Obtenus\n",
    "\n",
    "| Configuration | Val IoU | Score Test |\n",
    "|---------------|---------|------------|\n",
    "| 5-way 1-shot | 24.13% | 18.5% |\n",
    "| **5-way 5-shot** | **38.30%** | **32%** |\n",
    "\n",
    "**Top 3 du challenge: 34%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
